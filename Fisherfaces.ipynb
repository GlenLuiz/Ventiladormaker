{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EigenFaces.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxnwKSl6Cgi63UBwEX8LQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GlenLuiz/Ventiladormaker/blob/master/Fisherfaces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSDjx-53CcJY",
        "outputId": "0b9e99c1-73a1-4c19-e8a5-225cc41b4475"
      },
      "source": [
        "pip install opencv_contrib_python"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv_contrib_python in /usr/local/lib/python3.7/dist-packages (4.5.4.58)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv_contrib_python) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu3BCAyAM0DX",
        "outputId": "2ddf8bcb-a703-4810-edaa-1dae73858019"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6pWJ_lxCe1X"
      },
      "source": [
        "import cv2\n",
        "import imageio\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS2F6TpXEhsA"
      },
      "source": [
        "source_image = imageio.imread('/content/gdrive/My Drive/first-order-motion-model/far2.JPG')\n",
        "source_image = resize(source_image, (256, 256))[..., :3]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h45MN01QDhcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8a65e4-ba53-46d2-ea10-ca6331e8e5a8"
      },
      "source": [
        "print(help(cv2.face))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on module cv2.face in cv2:\n",
            "\n",
            "NAME\n",
            "    cv2.face\n",
            "\n",
            "FUNCTIONS\n",
            "    BIF_create(...)\n",
            "        BIF_create([, num_bands[, num_rotations]]) -> retval\n",
            "        .   * @param num_bands The number of filter bands (<=8) used for computing BIF.\n",
            "        .        * @param num_rotations The number of image rotations for computing BIF.\n",
            "        .        * @returns Object for computing BIF.\n",
            "    \n",
            "    EigenFaceRecognizer_create(...)\n",
            "        EigenFaceRecognizer_create([, num_components[, threshold]]) -> retval\n",
            "        .   @param num_components The number of components (read: Eigenfaces) kept for this Principal\n",
            "        .       Component Analysis. As a hint: There's no rule how many components (read: Eigenfaces) should be\n",
            "        .       kept for good reconstruction capabilities. It is based on your input data, so experiment with the\n",
            "        .       number. Keeping 80 components should almost always be sufficient.\n",
            "        .       @param threshold The threshold applied in the prediction.\n",
            "        .   \n",
            "        .       ### Notes:\n",
            "        .   \n",
            "        .       -   Training and prediction must be done on grayscale images, use cvtColor to convert between the\n",
            "        .           color spaces.\n",
            "        .       -   **THE EIGENFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL\n",
            "        .           SIZE.** (caps-lock, because I got so many mails asking for this). You have to make sure your\n",
            "        .           input data has the correct shape, else a meaningful exception is thrown. Use resize to resize\n",
            "        .           the images.\n",
            "        .       -   This model does not support updating.\n",
            "        .   \n",
            "        .       ### Model internal data:\n",
            "        .   \n",
            "        .       -   num_components see EigenFaceRecognizer::create.\n",
            "        .       -   threshold see EigenFaceRecognizer::create.\n",
            "        .       -   eigenvalues The eigenvalues for this Principal Component Analysis (ordered descending).\n",
            "        .       -   eigenvectors The eigenvectors for this Principal Component Analysis (ordered by their\n",
            "        .           eigenvalue).\n",
            "        .       -   mean The sample mean calculated from the training data.\n",
            "        .       -   projections The projections of the training data.\n",
            "        .       -   labels The threshold applied in the prediction. If the distance to the nearest neighbor is\n",
            "        .           larger than the threshold, this method returns -1.\n",
            "    \n",
            "    FisherFaceRecognizer_create(...)\n",
            "        FisherFaceRecognizer_create([, num_components[, threshold]]) -> retval\n",
            "        .   @param num_components The number of components (read: Fisherfaces) kept for this Linear\n",
            "        .       Discriminant Analysis with the Fisherfaces criterion. It's useful to keep all components, that\n",
            "        .       means the number of your classes c (read: subjects, persons you want to recognize). If you leave\n",
            "        .       this at the default (0) or set it to a value less-equal 0 or greater (c-1), it will be set to the\n",
            "        .       correct number (c-1) automatically.\n",
            "        .       @param threshold The threshold applied in the prediction. If the distance to the nearest neighbor\n",
            "        .       is larger than the threshold, this method returns -1.\n",
            "        .   \n",
            "        .       ### Notes:\n",
            "        .   \n",
            "        .       -   Training and prediction must be done on grayscale images, use cvtColor to convert between the\n",
            "        .           color spaces.\n",
            "        .       -   **THE FISHERFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL\n",
            "        .           SIZE.** (caps-lock, because I got so many mails asking for this). You have to make sure your\n",
            "        .           input data has the correct shape, else a meaningful exception is thrown. Use resize to resize\n",
            "        .           the images.\n",
            "        .       -   This model does not support updating.\n",
            "        .   \n",
            "        .       ### Model internal data:\n",
            "        .   \n",
            "        .       -   num_components see FisherFaceRecognizer::create.\n",
            "        .       -   threshold see FisherFaceRecognizer::create.\n",
            "        .       -   eigenvalues The eigenvalues for this Linear Discriminant Analysis (ordered descending).\n",
            "        .       -   eigenvectors The eigenvectors for this Linear Discriminant Analysis (ordered by their\n",
            "        .           eigenvalue).\n",
            "        .       -   mean The sample mean calculated from the training data.\n",
            "        .       -   projections The projections of the training data.\n",
            "        .       -   labels The labels corresponding to the projections.\n",
            "    \n",
            "    LBPHFaceRecognizer_create(...)\n",
            "        LBPHFaceRecognizer_create([, radius[, neighbors[, grid_x[, grid_y[, threshold]]]]]) -> retval\n",
            "        .   @param radius The radius used for building the Circular Local Binary Pattern. The greater the\n",
            "        .       radius, the smoother the image but more spatial information you can get.\n",
            "        .       @param neighbors The number of sample points to build a Circular Local Binary Pattern from. An\n",
            "        .       appropriate value is to use `8` sample points. Keep in mind: the more sample points you include,\n",
            "        .       the higher the computational cost.\n",
            "        .       @param grid_x The number of cells in the horizontal direction, 8 is a common value used in\n",
            "        .       publications. The more cells, the finer the grid, the higher the dimensionality of the resulting\n",
            "        .       feature vector.\n",
            "        .       @param grid_y The number of cells in the vertical direction, 8 is a common value used in\n",
            "        .       publications. The more cells, the finer the grid, the higher the dimensionality of the resulting\n",
            "        .       feature vector.\n",
            "        .       @param threshold The threshold applied in the prediction. If the distance to the nearest neighbor\n",
            "        .       is larger than the threshold, this method returns -1.\n",
            "        .   \n",
            "        .       ### Notes:\n",
            "        .   \n",
            "        .       -   The Circular Local Binary Patterns (used in training and prediction) expect the data given as\n",
            "        .           grayscale images, use cvtColor to convert between the color spaces.\n",
            "        .       -   This model supports updating.\n",
            "        .   \n",
            "        .       ### Model internal data:\n",
            "        .   \n",
            "        .       -   radius see LBPHFaceRecognizer::create.\n",
            "        .       -   neighbors see LBPHFaceRecognizer::create.\n",
            "        .       -   grid_x see LLBPHFaceRecognizer::create.\n",
            "        .       -   grid_y see LBPHFaceRecognizer::create.\n",
            "        .       -   threshold see LBPHFaceRecognizer::create.\n",
            "        .       -   histograms Local Binary Patterns Histograms calculated from the given training data (empty if\n",
            "        .           none was given).\n",
            "        .       -   labels Labels corresponding to the calculated Local Binary Patterns Histograms.\n",
            "    \n",
            "    MACE_create(...)\n",
            "        MACE_create([, IMGSIZE]) -> retval\n",
            "        .   @brief constructor\n",
            "        .       @param IMGSIZE  images will get resized to this (should be an even number)\n",
            "    \n",
            "    MACE_load(...)\n",
            "        MACE_load(filename[, objname]) -> retval\n",
            "        .   @brief constructor\n",
            "        .       @param filename  build a new MACE instance from a pre-serialized FileStorage\n",
            "        .       @param objname (optional) top-level node in the FileStorage\n",
            "    \n",
            "    StandardCollector_create(...)\n",
            "        StandardCollector_create([, threshold]) -> retval\n",
            "        .   @brief Static constructor\n",
            "        .       @param threshold set threshold\n",
            "    \n",
            "    createFacemarkAAM(...)\n",
            "        createFacemarkAAM() -> retval\n",
            "        .\n",
            "    \n",
            "    createFacemarkKazemi(...)\n",
            "        createFacemarkKazemi() -> retval\n",
            "        .\n",
            "    \n",
            "    createFacemarkLBF(...)\n",
            "        createFacemarkLBF() -> retval\n",
            "        .\n",
            "    \n",
            "    drawFacemarks(...)\n",
            "        drawFacemarks(image, points[, color]) -> image\n",
            "        .   @brief Utility to draw the detected facial landmark points\n",
            "        .   \n",
            "        .   @param image The input image to be processed.\n",
            "        .   @param points Contains the data of points which will be drawn.\n",
            "        .   @param color The color of points in BGR format represented by cv::Scalar.\n",
            "        .   \n",
            "        .   <B>Example of usage</B>\n",
            "        .   @code\n",
            "        .   std::vector<Rect> faces;\n",
            "        .   std::vector<std::vector<Point2f> > landmarks;\n",
            "        .   facemark->getFaces(img, faces);\n",
            "        .   facemark->fit(img, faces, landmarks);\n",
            "        .   for(int j=0;j<rects.size();j++){\n",
            "        .       face::drawFacemarks(frame, landmarks[j], Scalar(0,0,255));\n",
            "        .   }\n",
            "        .   @endcode\n",
            "    \n",
            "    getFacesHAAR(...)\n",
            "        getFacesHAAR(image, face_cascade_name[, faces]) -> retval, faces\n",
            "        .   @brief Default face detector\n",
            "        .   This function is mainly utilized by the implementation of a Facemark Algorithm.\n",
            "        .   End users are advised to use function Facemark::getFaces which can be manually defined\n",
            "        .   and circumvented to the algorithm by Facemark::setFaceDetector.\n",
            "        .   \n",
            "        .   @param image The input image to be processed.\n",
            "        .   @param faces Output of the function which represent region of interest of the detected faces.\n",
            "        .   Each face is stored in cv::Rect container.\n",
            "        .   @param params detector parameters\n",
            "        .   \n",
            "        .   <B>Example of usage</B>\n",
            "        .   @code\n",
            "        .   std::vector<cv::Rect> faces;\n",
            "        .   CParams params(\"haarcascade_frontalface_alt.xml\");\n",
            "        .   cv::face::getFaces(frame, faces, &params);\n",
            "        .   for(int j=0;j<faces.size();j++){\n",
            "        .       cv::rectangle(frame, faces[j], cv::Scalar(255,0,255));\n",
            "        .   }\n",
            "        .   cv::imshow(\"detection\", frame);\n",
            "        .   @endcode\n",
            "    \n",
            "    loadDatasetList(...)\n",
            "        loadDatasetList(imageList, annotationList, images, annotations) -> retval\n",
            "        .   @brief A utility to load list of paths to training image and annotation file.\n",
            "        .   @param imageList The specified file contains paths to the training images.\n",
            "        .   @param annotationList The specified file contains paths to the training annotations.\n",
            "        .   @param images The loaded paths of training images.\n",
            "        .   @param annotations The loaded paths of annotation files.\n",
            "        .   \n",
            "        .   Example of usage:\n",
            "        .   @code\n",
            "        .   String imageFiles = \"images_path.txt\";\n",
            "        .   String ptsFiles = \"annotations_path.txt\";\n",
            "        .   std::vector<String> images_train;\n",
            "        .   std::vector<String> landmarks_train;\n",
            "        .   loadDatasetList(imageFiles,ptsFiles,images_train,landmarks_train);\n",
            "        .   @endcode\n",
            "    \n",
            "    loadFacePoints(...)\n",
            "        loadFacePoints(filename[, points[, offset]]) -> retval, points\n",
            "        .   @brief A utility to load facial landmark information from a given file.\n",
            "        .   \n",
            "        .   @param filename The filename of file contains the facial landmarks data.\n",
            "        .   @param points The loaded facial landmark points.\n",
            "        .   @param offset An offset value to adjust the loaded points.\n",
            "        .   \n",
            "        .   <B>Example of usage</B>\n",
            "        .   @code\n",
            "        .   std::vector<Point2f> points;\n",
            "        .   face::loadFacePoints(\"filename.txt\", points, 0.0f);\n",
            "        .   @endcode\n",
            "        .   \n",
            "        .   The annotation file should follow the default format which is\n",
            "        .   @code\n",
            "        .   version: 1\n",
            "        .   n_points:  68\n",
            "        .   {\n",
            "        .   212.716603 499.771793\n",
            "        .   230.232816 566.290071\n",
            "        .   ...\n",
            "        .   }\n",
            "        .   @endcode\n",
            "        .   where n_points is the number of points considered\n",
            "        .   and each point is represented as its position in x and y.\n",
            "    \n",
            "    loadTrainingData(...)\n",
            "        loadTrainingData(filename, images[, facePoints[, delim[, offset]]]) -> retval, facePoints\n",
            "        .   @brief A utility to load facial landmark dataset from a single file.\n",
            "        .   \n",
            "        .   @param filename The filename of a file that contains the dataset information.\n",
            "        .   Each line contains the filename of an image followed by\n",
            "        .   pairs of x and y values of facial landmarks points separated by a space.\n",
            "        .   Example\n",
            "        .   @code\n",
            "        .   /home/user/ibug/image_003_1.jpg 336.820955 240.864510 334.238298 260.922709 335.266918 ...\n",
            "        .   /home/user/ibug/image_005_1.jpg 376.158428 230.845712 376.736984 254.924635 383.265403 ...\n",
            "        .   @endcode\n",
            "        .   @param images A vector where each element represent the filename of image in the dataset.\n",
            "        .   Images are not loaded by default to save the memory.\n",
            "        .   @param facePoints The loaded landmark points for all training data.\n",
            "        .   @param delim Delimiter between each element, the default value is a whitespace.\n",
            "        .   @param offset An offset value to adjust the loaded points.\n",
            "        .   \n",
            "        .   <B>Example of usage</B>\n",
            "        .   @code\n",
            "        .   cv::String imageFiles = \"../data/images_train.txt\";\n",
            "        .   cv::String ptsFiles = \"../data/points_train.txt\";\n",
            "        .   std::vector<String> images;\n",
            "        .   std::vector<std::vector<Point2f> > facePoints;\n",
            "        .   loadTrainingData(imageFiles, ptsFiles, images, facePoints, 0.0f);\n",
            "        .   @endcode\n",
            "        \n",
            "        \n",
            "        \n",
            "        loadTrainingData(imageList, groundTruth, images[, facePoints[, offset]]) -> retval, facePoints\n",
            "        .   @brief A utility to load facial landmark information from the dataset.\n",
            "        .   \n",
            "        .   @param imageList A file contains the list of image filenames in the training dataset.\n",
            "        .   @param groundTruth A file contains the list of filenames\n",
            "        .   where the landmarks points information are stored.\n",
            "        .   The content in each file should follow the standard format (see face::loadFacePoints).\n",
            "        .   @param images A vector where each element represent the filename of image in the dataset.\n",
            "        .   Images are not loaded by default to save the memory.\n",
            "        .   @param facePoints The loaded landmark points for all training data.\n",
            "        .   @param offset An offset value to adjust the loaded points.\n",
            "        .   \n",
            "        .   <B>Example of usage</B>\n",
            "        .   @code\n",
            "        .   cv::String imageFiles = \"../data/images_train.txt\";\n",
            "        .   cv::String ptsFiles = \"../data/points_train.txt\";\n",
            "        .   std::vector<String> images;\n",
            "        .   std::vector<std::vector<Point2f> > facePoints;\n",
            "        .   loadTrainingData(imageFiles, ptsFiles, images, facePoints, 0.0f);\n",
            "        .   @endcode\n",
            "        .   \n",
            "        .   example of content in the images_train.txt\n",
            "        .   @code\n",
            "        .   /home/user/ibug/image_003_1.jpg\n",
            "        .   /home/user/ibug/image_004_1.jpg\n",
            "        .   /home/user/ibug/image_005_1.jpg\n",
            "        .   /home/user/ibug/image_006.jpg\n",
            "        .   @endcode\n",
            "        .   \n",
            "        .   example of content in the points_train.txt\n",
            "        .   @code\n",
            "        .   /home/user/ibug/image_003_1.pts\n",
            "        .   /home/user/ibug/image_004_1.pts\n",
            "        .   /home/user/ibug/image_005_1.pts\n",
            "        .   /home/user/ibug/image_006.pts\n",
            "        .   @endcode\n",
            "        \n",
            "        \n",
            "        \n",
            "        loadTrainingData(filename, trainlandmarks, trainimages) -> retval\n",
            "        .   @brief This function extracts the data for training from .txt files which contains the corresponding image name and landmarks.\n",
            "        .   *The first file in each file should give the path of the image whose\n",
            "        .   *landmarks are being described in the file. Then in the subsequent\n",
            "        .   *lines there should be coordinates of the landmarks in the image\n",
            "        .   *i.e each line should be of the form x,y\n",
            "        .   *where x represents the x coordinate of the landmark and y represents\n",
            "        .   *the y coordinate of the landmark.\n",
            "        .   *\n",
            "        .   *For reference you can see the files as provided in the\n",
            "        .   *<a href=\"http://www.ifp.illinois.edu/~vuongle2/helen/\">HELEN dataset</a>\n",
            "        .   *\n",
            "        .   * @param filename A vector of type cv::String containing name of the .txt files.\n",
            "        .   * @param trainlandmarks A vector of type cv::Point2f that would store shape or landmarks of all images.\n",
            "        .   * @param trainimages A vector of type cv::String which stores the name of images whose landmarks are tracked\n",
            "        .   * @returns A boolean value. It returns true when it reads the data successfully and false otherwise\n",
            "\n",
            "FILE\n",
            "    (built-in)\n",
            "\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhDghYihCrc0"
      },
      "source": [
        "eigenfaces = cv2.face.EigenFaceRecognizer_create()"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iA_UX-6Nwe_"
      },
      "source": [
        "path = 'gdrive/My Drive/AlgoritimosOpencv'\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-bgm0JGD8q9"
      },
      "source": [
        "def getImage():\n",
        "    caminho = [os.path.join(path, f) for f in os.listdir(path)]\n",
        "    faces = []\n",
        "    ids = []\n",
        "    for caminhoImg in caminho:\n",
        "      imagemFace = cv2.cvtColor(cv2.imread(caminhoImg), cv2.COLOR_BGR2GRAY)\n",
        "      id = int(os.path.split(caminhoImg)[-1].split('.')[1])\n",
        "      #print(id)\n",
        "      ids.append(id)\n",
        "      faces.append(imagemFace)\n",
        "    return np.array(ids), faces"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwCQ3BhxNNAg"
      },
      "source": [
        "ids, faces = getImage()\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DfQxD7sRbR-",
        "outputId": "4e0e4d2d-7da0-4cfb-a114-4a54b1347be2"
      },
      "source": [
        "print(ids)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 2 2 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPIlEIntRfbK",
        "outputId": "f45f21df-36d4-431f-a6fc-a9983ad1e3e9"
      },
      "source": [
        "print(faces)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[205, 206, 207, ..., 230, 230, 231],\n",
            "       [205, 206, 207, ..., 230, 230, 231],\n",
            "       [205, 206, 207, ..., 227, 227, 228],\n",
            "       ...,\n",
            "       [145, 156, 186, ...,  72,  79,  67],\n",
            "       [145, 160, 190, ...,  61,  75,  71],\n",
            "       [133, 155, 179, ...,  51,  65,  69]], dtype=uint8), array([[196, 193, 193, ..., 237, 237, 236],\n",
            "       [194, 193, 194, ..., 238, 238, 237],\n",
            "       [193, 193, 193, ..., 238, 238, 237],\n",
            "       ...,\n",
            "       [ 31,  24,  23, ...,  34,  31,  33],\n",
            "       [ 28,  23,  23, ...,  32,  28,  30],\n",
            "       [ 43,  36,  36, ...,  31,  29,  33]], dtype=uint8), array([[212, 209, 209, ..., 218, 218, 221],\n",
            "       [208, 205, 204, ..., 215, 215, 218],\n",
            "       [209, 205, 204, ..., 216, 216, 220],\n",
            "       ...,\n",
            "       [163, 182, 187, ...,  43,  44,  57],\n",
            "       [167, 182, 187, ...,  42,  41,  52],\n",
            "       [177, 188, 190, ...,  55,  55,  70]], dtype=uint8), array([[215, 215, 215, ..., 229, 229, 230],\n",
            "       [216, 216, 216, ..., 229, 229, 229],\n",
            "       [216, 216, 216, ..., 229, 229, 229],\n",
            "       ...,\n",
            "       [ 51,  43,  43, ...,  51,  50,  50],\n",
            "       [ 47,  40,  40, ...,  49,  48,  48],\n",
            "       [ 57,  48,  47, ...,  48,  47,  49]], dtype=uint8), array([[165, 160, 157, ..., 166, 165, 164],\n",
            "       [165, 169, 171, ..., 165, 166, 164],\n",
            "       [161, 170, 171, ..., 163, 166, 164],\n",
            "       ...,\n",
            "       [ 13,  14,  13, ...,   6,   6,  14],\n",
            "       [ 12,  11,   9, ...,   5,   6,  13],\n",
            "       [  9,   9,   7, ...,  14,  16,  23]], dtype=uint8), array([[173, 172, 173, ..., 162, 162, 163],\n",
            "       [172, 172, 173, ..., 161, 161, 161],\n",
            "       [171, 171, 171, ..., 162, 162, 160],\n",
            "       ...,\n",
            "       [ 14,   9,  14, ...,   8,  11,  16],\n",
            "       [ 14,  12,  15, ...,   9,  13,  13],\n",
            "       [ 14,  12,  14, ...,   7,  13,  18]], dtype=uint8), array([[120,  39,  31, ..., 161, 160, 161],\n",
            "       [ 47,  16,  20, ..., 162, 160, 161],\n",
            "       [ 33,  17,  18, ..., 161, 162, 160],\n",
            "       ...,\n",
            "       [ 10,  14,  12, ..., 149, 147, 139],\n",
            "       [ 10,  10,   8, ..., 147, 146, 137],\n",
            "       [ 10,   9,   8, ..., 136, 138, 128]], dtype=uint8), array([[175, 177, 175, ..., 165, 165, 170],\n",
            "       [176, 175, 174, ..., 160, 161, 164],\n",
            "       [175, 174, 174, ..., 159, 160, 166],\n",
            "       ...,\n",
            "       [ 10,  10,   9, ...,  22,  52,  53],\n",
            "       [ 11,  11,  10, ...,  13,  48,  55],\n",
            "       [ 21,  19,  21, ...,  10,  42,  59]], dtype=uint8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftUzt4ckbX9-"
      },
      "source": [
        "Gerando classificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8rkElXmRn5P"
      },
      "source": [
        "eigenfaces.train(faces, ids)\n",
        "eigenfaces.write('classEigen.yml')"
      ],
      "execution_count": 83,
      "outputs": []
    }
  ]
}